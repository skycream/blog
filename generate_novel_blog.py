"""
ì°¨ë³„í™”ëœ ë¸”ë¡œê·¸ ìƒì„± íŒŒì´í”„ë¼ì¸

1. ê¸°ì¡´ ë¸”ë¡œê·¸ ë¶„ì„ â†’ ë­ê°€ ì´ë¯¸ ìˆëŠ”ì§€ íŒŒì•…
2. PubMed ë…¼ë¬¸ ê²€ìƒ‰
3. ë…¼ë¬¸ vs ê¸°ì¡´ ë¸”ë¡œê·¸ ë¹„êµ â†’ ìƒˆë¡œìš´ ì‚¬ì‹¤ ì¶”ì¶œ
4. ì°¨ë³„í™”ëœ ì½˜í…ì¸ ë¡œ ë¸”ë¡œê·¸ ìƒì„±
"""

import argparse
import json
import os
from datetime import datetime
from typing import List, Dict

# ëª¨ë“ˆ ì„í¬íŠ¸
from modules.competitor_analyzer import CompetitorAnalyzer, NoveltyFinder
from modules.pubmed_search import PubMedSearcher
from modules.paper_classifier import PaperClassifier
from config import Config


def run_pipeline(keyword: str,
                 korean_keyword: str = None,
                 max_blogs: int = 10,
                 max_papers: int = 50,
                 skip_competitor: bool = False) -> Dict:
    """
    ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

    Args:
        keyword: ì˜ë¬¸ í‚¤ì›Œë“œ (PubMed ê²€ìƒ‰ìš©)
        korean_keyword: í•œê¸€ í‚¤ì›Œë“œ (ë¸”ë¡œê·¸ ê²€ìƒ‰ìš©, ì—†ìœ¼ë©´ keyword ì‚¬ìš©)
        max_blogs: ë¶„ì„í•  ê²½ìŸ ë¸”ë¡œê·¸ ìˆ˜
        max_papers: ê²€ìƒ‰í•  ë…¼ë¬¸ ìˆ˜
        skip_competitor: ê²½ìŸ ë¶„ì„ ìŠ¤í‚µ (ì´ë¯¸ ë°ì´í„°ê°€ ìˆì„ ë•Œ)
    """

    korean_keyword = korean_keyword or keyword
    output_dir = "output"
    os.makedirs(output_dir, exist_ok=True)

    print("=" * 60)
    print(f"ì°¨ë³„í™” ë¸”ë¡œê·¸ ìƒì„± íŒŒì´í”„ë¼ì¸")
    print(f"ì˜ë¬¸ í‚¤ì›Œë“œ: {keyword}")
    print(f"í•œê¸€ í‚¤ì›Œë“œ: {korean_keyword}")
    print("=" * 60)

    # ============================================
    # STEP 1: ê²½ìŸ ë¸”ë¡œê·¸ ë¶„ì„
    # ============================================
    print("\n[STEP 1] ê²½ìŸ ë¸”ë¡œê·¸ ë¶„ì„")

    analyzer = CompetitorAnalyzer(output_dir="competitor_data")

    if skip_competitor:
        competitor_data = analyzer.load_competitor_data(korean_keyword)
        if competitor_data:
            print(f"  â†’ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ: {competitor_data['blog_count']}ê°œ ë¸”ë¡œê·¸")
        else:
            print("  â†’ ê¸°ì¡´ ë°ì´í„° ì—†ìŒ, ìƒˆë¡œ ë¶„ì„...")
            competitor_data = analyzer.search_and_analyze(korean_keyword, max_blogs)
    else:
        competitor_data = analyzer.search_and_analyze(korean_keyword, max_blogs)

    # ê³µí†µ í¬ì¸íŠ¸ ì¶œë ¥
    print("\n  [ê¸°ì¡´ ë¸”ë¡œê·¸ë“¤ì˜ ê³µí†µ ë‚´ìš©]")
    for point in competitor_data.get('common_points', []):
        print(f"    - {point}")

    # ============================================
    # STEP 2: PubMed ë…¼ë¬¸ ê²€ìƒ‰
    # ============================================
    print("\n[STEP 2] PubMed ë…¼ë¬¸ ê²€ìƒ‰")

    searcher = PubMedSearcher(email=Config.PUBMED_EMAIL)

    # ë‹¤ì–‘í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
    queries = [
        f"{keyword}",
        f"{keyword} treatment",
        f"{keyword} diet food",
        f"{keyword} lifestyle",
        f"{keyword} risk factors",
        f"{keyword} complications",
        f"{keyword} prevention",
        f"{keyword} novel findings",
        f"{keyword} recent advances",
        f"{keyword} meta-analysis",
    ]

    all_papers = []
    seen_pmids = set()

    for query in queries:
        papers = searcher.search(query, max_results=max_papers // len(queries))
        for paper in papers:
            if paper['pmid'] not in seen_pmids:
                seen_pmids.add(paper['pmid'])
                all_papers.append(paper)

    print(f"  â†’ ì´ {len(all_papers)}ê°œ ë…¼ë¬¸ ìˆ˜ì§‘")

    # ============================================
    # STEP 3: ìƒˆë¡œìš´ ì‚¬ì‹¤ ì°¾ê¸°
    # ============================================
    print("\n[STEP 3] ìƒˆë¡œìš´ ì‚¬ì‹¤ ì°¾ê¸° (ë…¼ë¬¸ vs ê¸°ì¡´ ë¸”ë¡œê·¸)")

    finder = NoveltyFinder()
    novel_facts = finder.find_novel_facts(all_papers, competitor_data, min_novelty_score=0.25)

    print(f"  â†’ {len(novel_facts)}ê°œ ìƒˆë¡œìš´ ì‚¬ì‹¤ ë°œê²¬")

    # ìš”ì•½
    summary = finder.summarize_novel_findings(novel_facts, top_n=30)

    print(f"\n  [ê°ì„±ë³„ ë¶„ë¥˜]")
    print(f"    - ê¸ì •ì  ë°œê²¬: {len(summary['by_sentiment']['positive'])}ê°œ")
    print(f"    - ë¶€ì •ì  ë°œê²¬: {len(summary['by_sentiment']['negative'])}ê°œ")
    print(f"    - ì¤‘ë¦½ì  ë°œê²¬: {len(summary['by_sentiment']['neutral'])}ê°œ")

    print(f"\n  [ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜]")
    for cat, facts in summary['by_category'].items():
        print(f"    - {cat}: {len(facts)}ê°œ")

    print(f"\n  [í†µê³„ ë°ì´í„° í¬í•¨ ë°œê²¬: {len(summary['statistics_findings'])}ê°œ]")

    # ============================================
    # STEP 4: ê²°ê³¼ ì €ì¥
    # ============================================
    print("\n[STEP 4] ê²°ê³¼ ì €ì¥")

    # ìƒˆë¡œìš´ ë°œê²¬ë“¤ ì €ì¥
    result = {
        'keyword': keyword,
        'korean_keyword': korean_keyword,
        'generated_at': datetime.now().isoformat(),
        'competitor_analysis': {
            'blog_count': competitor_data['blog_count'],
            'common_points': competitor_data['common_points'],
        },
        'papers_analyzed': len(all_papers),
        'novel_facts_summary': summary,
        'top_novel_facts': novel_facts[:50],
        'all_novel_facts': novel_facts,
    }

    output_path = os.path.join(output_dir, f"{keyword}_novel_facts_{datetime.now().strftime('%Y%m%d')}.json")
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"  â†’ ì €ì¥ ì™„ë£Œ: {output_path}")

    # ============================================
    # TOP ë°œê²¬ë“¤ ì¶œë ¥
    # ============================================
    print("\n" + "=" * 60)
    print("TOP ìƒˆë¡œìš´ ë°œê²¬ë“¤ (ê¸°ì¡´ ë¸”ë¡œê·¸ì— ì—†ëŠ” ë‚´ìš©)")
    print("=" * 60)

    for i, fact in enumerate(novel_facts[:15], 1):
        sentiment_emoji = {'positive': 'âœ…', 'negative': 'âš ï¸', 'neutral': 'ğŸ“Œ'}[fact['sentiment']]
        stats_mark = 'ğŸ“Š' if fact['has_statistics'] else ''

        print(f"\n{i}. {sentiment_emoji} {stats_mark} [{fact['category']}]")
        print(f"   {fact['fact'][:200]}...")
        print(f"   ì¶œì²˜: PMID {fact['source_pmid']}")
        print(f"   ìƒˆë¡œì›€ ì ìˆ˜: {fact['novelty_score']:.2f}")

    return result


def print_blog_outline(result: Dict):
    """ë¸”ë¡œê·¸ ê¸€ ì•„ì›ƒë¼ì¸ ìƒì„±"""

    print("\n" + "=" * 60)
    print("ì°¨ë³„í™” ë¸”ë¡œê·¸ ê¸€ ì•„ì›ƒë¼ì¸")
    print("=" * 60)

    facts = result['all_novel_facts']

    # ì¹´í…Œê³ ë¦¬ë³„ ì •ë¦¬
    categories = {
        'cause': 'ì›ì¸í¸ - ìƒˆë¡œìš´ ë°œê²¬',
        'treatment': 'ì¹˜ë£Œí¸ - ìƒˆë¡œìš´ ë°œê²¬',
        'diet': 'ì‹ì´ìš”ë²•í¸ - ìƒˆë¡œìš´ ë°œê²¬',
        'lifestyle': 'ìƒí™œìŠµê´€í¸ - ìƒˆë¡œìš´ ë°œê²¬',
        'complications': 'í•©ë³‘ì¦í¸ - ìƒˆë¡œìš´ ë°œê²¬',
        'prevention': 'ì˜ˆë°©í¸ - ìƒˆë¡œìš´ ë°œê²¬',
    }

    for cat_key, cat_name in categories.items():
        cat_facts = [f for f in facts if f['category'] == cat_key]

        if not cat_facts:
            continue

        print(f"\n### {cat_name}")

        # ê¸ì •ì  ë°œê²¬
        positive = [f for f in cat_facts if f['sentiment'] == 'positive']
        if positive:
            print("\n  [âœ… í¬ì†Œì‹]")
            for f in positive[:3]:
                print(f"    - {f['fact'][:150]}...")

        # ë¶€ì •ì  ë°œê²¬
        negative = [f for f in cat_facts if f['sentiment'] == 'negative']
        if negative:
            print("\n  [âš ï¸ ì£¼ì˜í•  ì ]")
            for f in negative[:3]:
                print(f"    - {f['fact'][:150]}...")

        # í†µê³„ ë°ì´í„°
        stats = [f for f in cat_facts if f['has_statistics']]
        if stats:
            print("\n  [ğŸ“Š êµ¬ì²´ì  ìˆ˜ì¹˜]")
            for f in stats[:3]:
                print(f"    - {f['fact'][:150]}...")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='ì°¨ë³„í™”ëœ ë¸”ë¡œê·¸ ìƒì„±')
    parser.add_argument('keyword', help='ì˜ë¬¸ í‚¤ì›Œë“œ (PubMed ê²€ìƒ‰ìš©)')
    parser.add_argument('--korean', '-k', help='í•œê¸€ í‚¤ì›Œë“œ (ë¸”ë¡œê·¸ ê²€ìƒ‰ìš©)')
    parser.add_argument('--blogs', type=int, default=10, help='ë¶„ì„í•  ê²½ìŸ ë¸”ë¡œê·¸ ìˆ˜')
    parser.add_argument('--papers', type=int, default=50, help='ê²€ìƒ‰í•  ë…¼ë¬¸ ìˆ˜')
    parser.add_argument('--skip-competitor', action='store_true', help='ê²½ìŸ ë¶„ì„ ìŠ¤í‚µ')
    parser.add_argument('--outline', action='store_true', help='ë¸”ë¡œê·¸ ì•„ì›ƒë¼ì¸ ì¶œë ¥')

    args = parser.parse_args()

    result = run_pipeline(
        keyword=args.keyword,
        korean_keyword=args.korean,
        max_blogs=args.blogs,
        max_papers=args.papers,
        skip_competitor=args.skip_competitor
    )

    if args.outline:
        print_blog_outline(result)
