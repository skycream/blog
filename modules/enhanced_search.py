"""
ê°•í™”ëœ PubMed ê²€ìƒ‰ ëª¨ë“ˆ
- í‚¤ì›Œë“œ í™•ì¥
- ë‹¤ì–‘í•œ ê²€ìƒ‰ì–´ ì¡°í•©
- ëŒ€ëŸ‰ ë…¼ë¬¸ ìˆ˜ì§‘
"""

import requests
import xml.etree.ElementTree as ET
from typing import List, Dict, Optional, Set
import time
import re
from config import Config


class EnhancedPubMedSearcher:
    """ê°•í™”ëœ PubMed ê²€ìƒ‰ê¸°"""

    def __init__(self, email: str = None):
        self.email = email or Config.PUBMED_EMAIL
        self.base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"

        # í‚¤ì›Œë“œ í™•ì¥ ì‚¬ì „
        self.keyword_expansions = {
            # ì—­ë¥˜ì„±ì‹ë„ì—¼
            'GERD': ['GERD', 'gastroesophageal reflux', 'acid reflux', 'heartburn',
                     'esophagitis', 'LES dysfunction', 'Barrett esophagus'],
            'ì—­ë¥˜ì„±ì‹ë„ì—¼': ['GERD', 'gastroesophageal reflux', 'acid reflux', 'reflux esophagitis'],

            # ì˜¤ë Œì§€ì£¼ìŠ¤
            'orange juice': ['orange juice', 'citrus juice', 'fruit juice',
                            'citrus consumption', 'orange beverage'],
            'ì˜¤ë Œì§€ì£¼ìŠ¤': ['orange juice', 'citrus juice', 'fruit juice'],

            # ëŸ¬ë‹
            'running': ['running', 'jogging', 'aerobic exercise', 'endurance running',
                       'recreational running', 'distance running'],
            'ëŸ¬ë‹': ['running', 'jogging', 'aerobic exercise'],

            # ì¼ë°˜ì ì¸ ê±´ê°• í‚¤ì›Œë“œ í™•ì¥
            'health': ['health effects', 'health benefits', 'health outcomes', 'health risks'],
            'treatment': ['treatment', 'therapy', 'management', 'intervention'],
            'diet': ['diet', 'dietary', 'nutrition', 'food', 'consumption'],
        }

        # ì£¼ì œë³„ ì„¸ë¶€ ê²€ìƒ‰ì–´
        self.topic_modifiers = {
            'general': ['', 'systematic review', 'meta-analysis', 'clinical trial'],
            'effects': ['effects', 'benefits', 'risks', 'outcomes'],
            'mechanism': ['mechanism', 'pathophysiology', 'etiology'],
            'epidemiology': ['prevalence', 'incidence', 'risk factors', 'epidemiology'],
        }

    def search_comprehensive(self, keyword: str,
                            max_total: int = 200,
                            years: int = 15) -> List[Dict]:
        """
        ì¢…í•©ì ì¸ ê²€ìƒ‰ ìˆ˜í–‰

        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            max_total: ìµœëŒ€ ìˆ˜ì§‘ ë…¼ë¬¸ ìˆ˜
            years: ìµœê·¼ ëª‡ ë…„ ì´ë‚´

        Returns:
            ë…¼ë¬¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        print(f"\n{'='*60}")
        print(f"ì¢…í•© ê²€ìƒ‰: {keyword}")
        print(f"ëª©í‘œ: {max_total}ê°œ ë…¼ë¬¸")
        print('='*60)

        # 1. í‚¤ì›Œë“œ í™•ì¥
        expanded_keywords = self._expand_keyword(keyword)
        print(f"\ní™•ì¥ëœ í‚¤ì›Œë“œ: {expanded_keywords}")

        # 2. ë‹¤ì–‘í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
        queries = self._generate_queries(expanded_keywords, years)
        print(f"ìƒì„±ëœ ì¿¼ë¦¬ ìˆ˜: {len(queries)}ê°œ")

        # 3. ê²€ìƒ‰ ì‹¤í–‰ ë° PMID ìˆ˜ì§‘
        all_pmids: Set[str] = set()
        per_query_limit = max(20, max_total // len(queries))

        for i, query in enumerate(queries):
            if len(all_pmids) >= max_total:
                break

            pmids = self._search_pmids(query, per_query_limit)
            new_pmids = set(pmids) - all_pmids
            all_pmids.update(new_pmids)

            if new_pmids:
                print(f"  [{i+1}/{len(queries)}] +{len(new_pmids)}ê°œ (ì´ {len(all_pmids)}ê°œ)")

            time.sleep(0.35)  # API ì œí•œ ì¤€ìˆ˜

        print(f"\nìˆ˜ì§‘ëœ ê³ ìœ  PMID: {len(all_pmids)}ê°œ")

        # 4. ë…¼ë¬¸ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        pmid_list = list(all_pmids)[:max_total]
        papers = self._fetch_details(pmid_list)

        print(f"ìµœì¢… ìˆ˜ì§‘ ë…¼ë¬¸: {len(papers)}ê°œ")

        return papers

    def _expand_keyword(self, keyword: str) -> List[str]:
        """í‚¤ì›Œë“œ í™•ì¥"""
        expanded = [keyword]

        # ì‚¬ì „ì—ì„œ í™•ì¥ í‚¤ì›Œë“œ ì°¾ê¸°
        keyword_lower = keyword.lower()
        for key, expansions in self.keyword_expansions.items():
            if key.lower() in keyword_lower or keyword_lower in key.lower():
                expanded.extend(expansions)

        # ì¤‘ë³µ ì œê±°
        return list(set(expanded))

    def _generate_queries(self, keywords: List[str], years: int) -> List[str]:
        """ë‹¤ì–‘í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±"""
        queries = []

        base_filter = f'AND (hasabstract[text]) AND ("last {years} years"[PDat]) AND (humans[MeSH Terms])'

        for kw in keywords:
            # ê¸°ë³¸ ê²€ìƒ‰
            queries.append(f'({kw}[Title/Abstract]) {base_filter}')

            # ê³ í’ˆì§ˆ ì—°êµ¬ íƒ€ì… ì¶”ê°€
            for study_type in ['systematic review', 'meta-analysis', 'randomized controlled trial']:
                queries.append(f'({kw}[Title/Abstract]) AND ({study_type}[PT]) {base_filter}')

        # ì¤‘ë³µ ì œê±°
        return list(set(queries))

    def _search_pmids(self, query: str, max_results: int) -> List[str]:
        """PMID ê²€ìƒ‰"""
        try:
            params = {
                'db': 'pubmed',
                'term': query,
                'retmax': max_results,
                'retmode': 'xml',
                'email': self.email
            }

            response = requests.get(f"{self.base_url}esearch.fcgi", params=params, timeout=15)
            response.raise_for_status()

            root = ET.fromstring(response.content)
            pmids = [id_elem.text for id_elem in root.findall('.//Id')]

            return pmids

        except Exception as e:
            print(f"    ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def _fetch_details(self, pmids: List[str], batch_size: int = 20) -> List[Dict]:
        """ë…¼ë¬¸ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        papers = []

        print(f"\në…¼ë¬¸ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")

        for i in range(0, len(pmids), batch_size):
            batch = pmids[i:i+batch_size]

            try:
                params = {
                    'db': 'pubmed',
                    'id': ','.join(batch),
                    'retmode': 'xml',
                    'email': self.email
                }

                response = requests.get(f"{self.base_url}efetch.fcgi", params=params, timeout=30)
                response.raise_for_status()

                root = ET.fromstring(response.content)

                for article in root.findall('.//PubmedArticle'):
                    paper = self._parse_article(article)
                    if paper:
                        papers.append(paper)

                print(f"  {len(papers)}/{len(pmids)} ì™„ë£Œ")
                time.sleep(0.35)

            except Exception as e:
                print(f"  ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue

        return papers

    def _parse_article(self, article) -> Optional[Dict]:
        """XMLì—ì„œ ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ"""
        try:
            pmid = article.findtext('.//PMID', 'Unknown')
            title = article.findtext('.//ArticleTitle', 'No Title')

            # ì €ì
            authors = []
            for author in article.findall('.//Author')[:5]:
                lastname = author.findtext('LastName', '')
                initials = author.findtext('Initials', '')
                if lastname:
                    authors.append(f"{lastname} {initials}".strip())

            journal = article.findtext('.//Journal/Title', 'Unknown')
            year = article.findtext('.//PubDate/Year', 'N/A')

            # ì´ˆë¡
            abstract_parts = []
            for elem in article.findall('.//AbstractText'):
                if elem.text:
                    label = elem.get('Label', '')
                    text = elem.text
                    if label:
                        abstract_parts.append(f"{label}: {text}")
                    else:
                        abstract_parts.append(text)

            abstract = ' '.join(abstract_parts) if abstract_parts else ''

            # ë…¼ë¬¸ íƒ€ì…
            pub_types = [pt.text for pt in article.findall('.//PublicationType') if pt.text]

            return {
                'pmid': pmid,
                'title': title,
                'authors': authors,
                'journal': journal,
                'year': year,
                'abstract': abstract,
                'pub_types': pub_types,
                'url': f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"
            }

        except Exception as e:
            return None

    def extract_statistics(self, papers: List[Dict]) -> List[Dict]:
        """ë…¼ë¬¸ì—ì„œ í†µê³„ ë°ì´í„° ì¶”ì¶œ"""
        stats_findings = []

        stat_patterns = [
            r'(\d+\.?\d*)\s*%',  # í¼ì„¼íŠ¸
            r'(\d+\.?\d*)-fold',  # ë°°ìˆ˜
            r'[pP]\s*[<>=]\s*(0\.\d+)',  # p-value
            r'OR\s*[=:]\s*(\d+\.?\d*)',  # Odds Ratio
            r'RR\s*[=:]\s*(\d+\.?\d*)',  # Relative Risk
            r'HR\s*[=:]\s*(\d+\.?\d*)',  # Hazard Ratio
            r'CI\s*[=:]\s*(\d+\.?\d*)',  # Confidence Interval
            r'(\d+\.?\d*)\s*times',  # N times
            r'reduced?\s+by\s+(\d+\.?\d*)\s*%',  # reduced by X%
            r'increase[d]?\s+by\s+(\d+\.?\d*)\s*%',  # increased by X%
        ]

        for paper in papers:
            abstract = paper.get('abstract', '')
            if not abstract:
                continue

            # ë¬¸ì¥ ë¶„ë¦¬
            sentences = re.split(r'(?<=[.!?])\s+', abstract)

            for sentence in sentences:
                # í†µê³„ íŒ¨í„´ ê²€ìƒ‰
                has_stat = False
                for pattern in stat_patterns:
                    if re.search(pattern, sentence, re.IGNORECASE):
                        has_stat = True
                        break

                if has_stat and 40 < len(sentence) < 500:
                    stats_findings.append({
                        'sentence': sentence,
                        'pmid': paper['pmid'],
                        'title': paper['title'],
                        'year': paper['year'],
                        'journal': paper['journal']
                    })

        # ì—°ë„ ê¸°ì¤€ ì •ë ¬ (ìµœì‹ ìˆœ)
        stats_findings.sort(key=lambda x: x.get('year', '0'), reverse=True)

        return stats_findings


def test_enhanced_search():
    """í…ŒìŠ¤íŠ¸"""
    searcher = EnhancedPubMedSearcher()

    keywords = ['GERD', 'orange juice', 'running']

    for kw in keywords:
        papers = searcher.search_comprehensive(kw, max_total=100)

        # í†µê³„ ì¶”ì¶œ
        stats = searcher.extract_statistics(papers)

        print(f"\n{'='*40}")
        print(f"{kw}: {len(papers)}ê°œ ë…¼ë¬¸, {len(stats)}ê°œ í†µê³„ ë°œê²¬")
        print('='*40)

        for stat in stats[:5]:
            print(f"\nğŸ“Š {stat['sentence'][:150]}...")
            print(f"   â†’ {stat['journal']} ({stat['year']}), PMID: {stat['pmid']}")


if __name__ == "__main__":
    test_enhanced_search()
