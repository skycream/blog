"""
LLM ê¸°ë°˜ í† í”½ ì¶”ì¶œ ëª¨ë“ˆ
- ë¸”ë¡œê·¸ ë³¸ë¬¸ì„ Claude CLIê°€ ë¶„ì„í•˜ì—¬ í•µì‹¬ í† í”½ ì¶”ì¶œ
- í•˜ë“œì½”ë”© ì‚¬ì „ ì—†ì´ ììœ ë¡œìš´ í† í”½ ë°œê²¬
"""

import os
import json
from datetime import datetime
from typing import List, Dict


def create_topic_extraction_prompt(blogs: List[Dict], keyword: str) -> str:
    """
    ë¸”ë¡œê·¸ ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±
    Claude CLIê°€ ê° ë¸”ë¡œê·¸ì˜ í•µì‹¬ í† í”½ì„ ì¶”ì¶œí•˜ë„ë¡ í•¨
    """

    blogs_text = ""
    for i, blog in enumerate(blogs, 1):
        content_preview = blog.get('content', '')[:2000]  # ê° ë¸”ë¡œê·¸ 2000ìê¹Œì§€
        blogs_text += f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“ ë¸”ë¡œê·¸ #{i}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ì œëª©: {blog.get('title', 'N/A')}
URL: {blog.get('url', 'N/A')}

ë³¸ë¬¸:
{content_preview}

"""

    prompt = f"""
# ë¸”ë¡œê·¸ í† í”½ ì¶”ì¶œ ìš”ì²­

## ê²€ìƒ‰ í‚¤ì›Œë“œ: {keyword}
## ë¶„ì„í•  ë¸”ë¡œê·¸ ìˆ˜: {len(blogs)}ê°œ

ë‹¹ì‹ ì€ ì½˜í…ì¸  ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ë¸”ë¡œê·¸ë“¤ì„ ë¶„ì„í•˜ì—¬ ê° ë¸”ë¡œê·¸ê°€ ê°•ì¡°í•˜ëŠ” **í•µì‹¬ í† í”½/í‚¤ì›Œë“œ**ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

{blogs_text}

## ì¶”ì¶œ ì§€ì¹¨

1. **ê° ë¸”ë¡œê·¸ë³„ë¡œ** ê·¸ ë¸”ë¡œê·¸ê°€ íŠ¹íˆ ê°•ì¡°í•˜ê±°ë‚˜ ë‹¤ë£¨ëŠ” í•µì‹¬ í¬ì¸íŠ¸ë¥¼ 2-5ê°œ ì¶”ì¶œ
2. **ìƒˆë¡œìš´ íŠ¸ë Œë“œ** í‚¤ì›Œë“œë„ ë°œê²¬í•´ì£¼ì„¸ìš” (ì˜ˆ: ìœ„ê³ ë¹„, ì˜¤ì ¬í”½, ì„¸ë§ˆê¸€ë£¨íƒ€ì´ë“œ ë“± ì‹ ì•½/ì‹ ê¸°ìˆ )
3. **êµ¬ì²´ì ì¸ í‚¤ì›Œë“œ**ë¡œ ì¶”ì¶œ (ì˜ˆ: "ì‹ì´ìš”ë²•" ë³´ë‹¤ "16:8 ë‹¨ì‹", "OMAD" ë“±)
4. **ì‹¤ìš©ì ì¸ í† í”½** ìš°ì„  (ë…ìë“¤ì´ ê´€ì‹¬ ê°€ì§ˆ ë§Œí•œ ê²ƒ)
5. í‚¤ì›Œë“œëŠ” **í•œê¸€**ë¡œ í†µì¼

## ì¶œë ¥ í˜•ì‹

JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”:

```json
{{
  "keyword": "{keyword}",
  "total_blogs": {len(blogs)},
  "blog_topics": [
    {{
      "blog_number": 1,
      "title": "ë¸”ë¡œê·¸ ì œëª©",
      "main_topics": ["í† í”½1", "í† í”½2", "í† í”½3"],
      "trend_keywords": ["ìƒˆë¡œìš´ íŠ¸ë Œë“œ í‚¤ì›Œë“œê°€ ìˆë‹¤ë©´"],
      "emphasis": "ì´ ë¸”ë¡œê·¸ê°€ íŠ¹íˆ ê°•ì¡°í•˜ëŠ” í¬ì¸íŠ¸ í•œ ë¬¸ì¥"
    }},
    ...
  ],
  "aggregated_topics": {{
    "high_frequency": ["ì—¬ëŸ¬ ë¸”ë¡œê·¸ì—ì„œ ê³µí†µìœ¼ë¡œ ì–¸ê¸‰ëœ í† í”½ë“¤"],
    "trending": ["ìƒˆë¡­ê±°ë‚˜ íŠ¸ë Œë””í•œ í‚¤ì›Œë“œë“¤"],
    "practical": ["ì‹¤ìš©ì ì¸ ì¡°ì–¸/ë°©ë²• ê´€ë ¨ í† í”½ë“¤"],
    "medical": ["ì˜í•™ì /ì „ë¬¸ì  í† í”½ë“¤"]
  }},
  "top_10_topics": [
    {{"topic": "í† í”½ëª…", "count": ì–¸ê¸‰ëœ_ë¸”ë¡œê·¸_ìˆ˜, "category": "ì¹´í…Œê³ ë¦¬"}},
    ...
  ],
  "recommended_pubmed_queries": [
    "ì˜ì–´ë¡œ ëœ PubMed ê²€ìƒ‰ ì¿¼ë¦¬ ì œì•ˆ (í‚¤ì›Œë“œ + í† í”½ ì¡°í•©)",
    ...
  ]
}}
```

JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
"""

    return prompt


def save_blogs_for_analysis(blogs: List[Dict], keyword: str, output_dir: str = "topic_data") -> str:
    """
    ë¸”ë¡œê·¸ ë°ì´í„°ë¥¼ Claude CLI ë¶„ì„ìš©ìœ¼ë¡œ ì €ì¥
    """
    os.makedirs(output_dir, exist_ok=True)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"{keyword}_blogs_{timestamp}.json"
    filepath = os.path.join(output_dir, filename)

    # ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±
    analysis_prompt = create_topic_extraction_prompt(blogs, keyword)

    data = {
        "keyword": keyword,
        "created_at": datetime.now().isoformat(),
        "total_blogs": len(blogs),
        "analysis_prompt": analysis_prompt,
        "blogs": blogs
    }

    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    return filepath


def parse_topic_extraction_result(json_text: str) -> Dict:
    """
    Claudeê°€ ë°˜í™˜í•œ í† í”½ ì¶”ì¶œ ê²°ê³¼ íŒŒì‹±
    """
    try:
        # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
        start = json_text.find('{')
        end = json_text.rfind('}') + 1
        if start != -1 and end > start:
            json_str = json_text[start:end]
            return json.loads(json_str)
    except json.JSONDecodeError:
        pass

    return {}


# í•œê¸€ í† í”½ â†’ ì˜ì–´ PubMed ì¿¼ë¦¬ ë³€í™˜ìš© í”„ë¡¬í”„íŠ¸
def create_topic_translation_prompt(topics: List[str], main_keyword_en: str) -> str:
    """
    ì¶”ì¶œëœ í•œê¸€ í† í”½ì„ PubMed ê²€ìƒ‰ìš© ì˜ì–´ë¡œ ë³€í™˜í•˜ëŠ” í”„ë¡¬í”„íŠ¸
    """
    topics_str = "\n".join([f"- {t}" for t in topics])

    return f"""
ë‹¤ìŒ í•œê¸€ í† í”½ë“¤ì„ PubMed ê²€ìƒ‰ì— ì í•©í•œ ì˜ì–´ ì˜í•™ ìš©ì–´ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.

ë©”ì¸ í‚¤ì›Œë“œ (ì˜ì–´): {main_keyword_en}

ë³€í™˜í•  í† í”½ë“¤:
{topics_str}

## ì¶œë ¥ í˜•ì‹

JSONìœ¼ë¡œ ë°˜í™˜:
```json
{{
  "translations": [
    {{"korean": "í•œê¸€ í† í”½", "english": "English medical term", "pubmed_query": "{main_keyword_en} AND english_term"}}
  ]
}}
```

## ì§€ì¹¨
1. ì˜í•™/ê³¼í•™ ë…¼ë¬¸ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì •í™•í•œ ì˜ì–´ ìš©ì–´ ì‚¬ìš©
2. ì•½í’ˆëª…ì€ ì„±ë¶„ëª…(generic name) ì‚¬ìš© (ì˜ˆ: ìœ„ê³ ë¹„ â†’ semaglutide)
3. ê²€ìƒ‰ íš¨ìœ¨ì„ ìœ„í•´ ë™ì˜ì–´ë„ í¬í•¨ ê°€ëŠ¥ (ì˜ˆ: "obesity OR overweight")
"""
