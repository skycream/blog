"""
ë¸”ë¡œê·¸ ìë™ ìƒì„± íŒŒì´í”„ë¼ì¸

ì‹¤í–‰ ë°©ë²•:
    python auto_pipeline.py "ì—­ë¥˜ì„±ì‹ë„ì—¼"

íŒŒì´í”„ë¼ì¸ íë¦„:
    [1] ì¸ê¸° ë¸”ë¡œê·¸ ìˆ˜ì§‘ (ë„¤ì´ë²„)
    [2] LLM/ê·œì¹™ ê¸°ë°˜ í† í”½ ì¶”ì¶œ (ë…ì ê´€ì‹¬ì‚¬ ë¶„ì„)
    [3] í† í”½ë³„ PubMed ë…¼ë¬¸ ê²€ìƒ‰
    [4] ì°¨ë³„í™” í¬ì¸íŠ¸ ì¶”ì¶œ
    [5] HTML ë¸”ë¡œê·¸ ê¸€ ìƒì„±
"""

import sys
import os
import json
from datetime import datetime

# ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from modules.smart_topic_extractor import SmartTopicExtractor
from modules.pubmed_search import PubMedSearcher
from config import Config


def run_pipeline(keyword: str, max_blogs: int = 20, max_papers: int = 50):
    """
    ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

    Args:
        keyword: ë©”ì¸ í‚¤ì›Œë“œ (ì˜ˆ: "ì—­ë¥˜ì„±ì‹ë„ì—¼")
        max_blogs: ë¶„ì„í•  ë¸”ë¡œê·¸ ìˆ˜
        max_papers: ìˆ˜ì§‘í•  ë…¼ë¬¸ ìˆ˜
    """
    print(f"\n{'#'*70}")
    print(f"# ë¸”ë¡œê·¸ ìë™ ìƒì„± íŒŒì´í”„ë¼ì¸")
    print(f"# í‚¤ì›Œë“œ: {keyword}")
    print(f"# ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'#'*70}")

    # ================================================================
    # STEP 1: ì¸ê¸° ë¸”ë¡œê·¸ì—ì„œ í† í”½ ì¶”ì¶œ
    # ================================================================
    print(f"\n\n{'='*70}")
    print(f"STEP 1: ì¸ê¸° ë¸”ë¡œê·¸ ë¶„ì„ â†’ ë…ì ê´€ì‹¬ í† í”½ ì¶”ì¶œ")
    print(f"{'='*70}")

    extractor = SmartTopicExtractor()
    topic_result = extractor.extract_topics(keyword, max_blogs=max_blogs)

    top_topics = topic_result['top_topics'][:10]  # ìƒìœ„ 10ê°œ í† í”½
    search_queries = topic_result['search_queries'][:10]

    print(f"\nğŸ“Š ìƒìœ„ ë…ì ê´€ì‹¬ í† í”½:")
    for i, topic in enumerate(top_topics, 1):
        count = topic_result['topic_counts'].get(topic, 0)
        print(f"   {i}. {topic} ({count}íšŒ ì–¸ê¸‰)")

    # ================================================================
    # STEP 2: PubMed ë…¼ë¬¸ ê²€ìƒ‰
    # ================================================================
    print(f"\n\n{'='*70}")
    print(f"STEP 2: PubMed ë…¼ë¬¸ ê²€ìƒ‰")
    print(f"{'='*70}")

    searcher = PubMedSearcher(
        email=Config.PUBMED_EMAIL,
        api_key=Config.PUBMED_API_KEY
    )

    all_papers = []
    papers_by_topic = {}

    for query in search_queries:
        print(f"\n  ê²€ìƒ‰: {query}")
        try:
            # search_and_fetchë¡œ ìƒì„¸ ì •ë³´ê¹Œì§€ ê°€ì ¸ì˜¤ê¸°
            papers = searcher.search_and_fetch(
                query=query,
                max_results=max_papers // len(search_queries) + 5
            )
            print(f"    â†’ {len(papers)}í¸ ìˆ˜ì§‘ ì™„ë£Œ")

            # í† í”½ë³„ ë¶„ë¥˜
            topic = query.split(' AND ')[1] if ' AND ' in query else query
            papers_by_topic[topic] = papers
            all_papers.extend(papers)

        except Exception as e:
            print(f"    â†’ ì˜¤ë¥˜: {e}")

    # ì¤‘ë³µ ì œê±°
    seen_pmids = set()
    unique_papers = []
    for paper in all_papers:
        pmid = paper.get('pmid')
        if pmid and pmid not in seen_pmids:
            seen_pmids.add(pmid)
            unique_papers.append(paper)

    print(f"\nğŸ“š ì´ {len(unique_papers)}í¸ ê³ ìœ  ë…¼ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ")

    # ================================================================
    # STEP 3: ê²°ê³¼ ì €ì¥
    # ================================================================
    print(f"\n\n{'='*70}")
    print(f"STEP 3: ê²°ê³¼ ì €ì¥")
    print(f"{'='*70}")

    # íŒŒì´í”„ë¼ì¸ ê²°ê³¼ ì €ì¥
    result = {
        'keyword': keyword,
        'keyword_en': topic_result.get('main_keyword_en', keyword),
        'pipeline_date': datetime.now().isoformat(),
        'blogs_analyzed': topic_result['blogs_analyzed'],
        'top_topics': top_topics,
        'topic_counts': topic_result['topic_counts'],
        'search_queries': search_queries,
        'papers_count': len(unique_papers),
        'papers_by_topic': {k: len(v) for k, v in papers_by_topic.items()},
        'papers': unique_papers[:100]  # ìƒìœ„ 100í¸ë§Œ ì €ì¥
    }

    # ì €ì¥
    output_dir = "pipeline_data"
    os.makedirs(output_dir, exist_ok=True)
    save_path = os.path.join(
        output_dir,
        f"{keyword}_pipeline_{datetime.now().strftime('%Y%m%d')}.json"
    )

    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {save_path}")

    # ================================================================
    # ìš”ì•½ ì¶œë ¥
    # ================================================================
    print(f"\n\n{'='*70}")
    print(f"ğŸ“‹ íŒŒì´í”„ë¼ì¸ ìš”ì•½")
    print(f"{'='*70}")
    print(f"  â€¢ í‚¤ì›Œë“œ: {keyword}")
    print(f"  â€¢ ë¶„ì„ ë¸”ë¡œê·¸: {topic_result['blogs_analyzed']}ê°œ")
    print(f"  â€¢ ë°œê²¬ í† í”½: {len(topic_result['topic_counts'])}ê°œ")
    print(f"  â€¢ ìˆ˜ì§‘ ë…¼ë¬¸: {len(unique_papers)}í¸")
    print(f"  â€¢ ì €ì¥ ê²½ë¡œ: {save_path}")

    print(f"\nğŸ“Œ ë‹¤ìŒ ë‹¨ê³„:")
    print(f"   ì´ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¸”ë¡œê·¸ ê¸€ì„ ì‘ì„±í•˜ì„¸ìš”.")
    print(f"   í† í”½ë³„ ë…¼ë¬¸ ë¶„ì„ ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ ì°¨ë³„í™”ëœ ì½˜í…ì¸ ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    # í† í”½ë³„ ë…¼ë¬¸ ìˆ˜ ì¶œë ¥
    print(f"\nğŸ“Š í† í”½ë³„ ë…¼ë¬¸ ìˆ˜:")
    for topic, count in sorted(papers_by_topic.items(), key=lambda x: -len(x[1])):
        print(f"   â€¢ {topic}: {len(count)}í¸")

    return result


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python auto_pipeline.py <í‚¤ì›Œë“œ>")
        print("ì˜ˆì‹œ: python auto_pipeline.py ì—­ë¥˜ì„±ì‹ë„ì—¼")
        sys.exit(1)

    keyword = sys.argv[1]
    run_pipeline(keyword)
