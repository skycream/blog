#!/usr/bin/env python
"""
ì‹œë¦¬ì¦ˆ ë¸”ë¡œê·¸ ìë™ ìƒì„± íŒŒì´í”„ë¼ì¸

í‚¤ì›Œë“œ ì…ë ¥ â†’ ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì—°ê´€ í‚¤ì›Œë“œ ì¶”ì¶œ â†’ PubMed ë…¼ë¬¸ ê²€ìƒ‰
â†’ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜ â†’ ì‹œë¦¬ì¦ˆ ë¸”ë¡œê·¸ ê¸€ ìƒì„±

ì‚¬ìš©ë²•:
    python generate_series.py "ì—­ë¥˜ì„±ì‹ë„ì—¼" --total 50
    python generate_series.py "ë¹„íƒ€ë¯¼D ê²°í•" --categories cause,treatment,diet
    python generate_series.py "GERD" --skip-web-search
"""

import argparse
import sys
import os
import json
from datetime import datetime
from typing import List, Dict, Optional

# ëª¨ë“ˆ ì„í¬íŠ¸
from modules.pubmed_search import PubMedSearcher
from modules.web_search import WebSearchKeywordExtractor
from modules.paper_classifier import PaperClassifier
from modules.series_generator import SeriesBlogGenerator
from config import Config


def search_with_related_keywords(
    searcher: PubMedSearcher,
    keyword_extractor: WebSearchKeywordExtractor,
    topic: str,
    total_papers: int,
    skip_web_search: bool = False
) -> List[Dict]:
    """
    ì—°ê´€ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ì—¬ ë…¼ë¬¸ ê²€ìƒ‰

    Args:
        searcher: PubMed ê²€ìƒ‰ê¸°
        keyword_extractor: í‚¤ì›Œë“œ ì¶”ì¶œê¸°
        topic: ê²€ìƒ‰ ì£¼ì œ
        total_papers: ëª©í‘œ ë…¼ë¬¸ ê°œìˆ˜
        skip_web_search: ì›¹ ê²€ìƒ‰ ê±´ë„ˆë›°ê¸°

    Returns:
        ìˆ˜ì§‘ëœ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸
    """
    all_papers = []
    seen_pmids = set()

    # 1. ì—°ê´€ í‚¤ì›Œë“œ ì¶”ì¶œ (ì˜µì…˜)
    if not skip_web_search:
        related_keywords = keyword_extractor.extract_related_keywords(
            topic,
            max_keywords=Config.MAX_RELATED_KEYWORDS
        )
        search_queries = keyword_extractor.get_search_queries(topic, related_keywords)
    else:
        print(f"\nğŸ” ì›¹ ê²€ìƒ‰ ê±´ë„ˆëœ€, ê¸°ë³¸ ì¿¼ë¦¬ë§Œ ì‚¬ìš©")
        search_queries = create_default_queries(topic)

    papers_per_query = max(5, total_papers // len(search_queries))

    print(f"\nğŸ“š {len(search_queries)}ê°œ ì¿¼ë¦¬ë¡œ ë…¼ë¬¸ ê²€ìƒ‰ ì‹œì‘...")
    print("-" * 70)

    # 2. ê° ì¿¼ë¦¬ë¡œ ë…¼ë¬¸ ê²€ìƒ‰
    for i, query in enumerate(search_queries, 1):
        if len(all_papers) >= total_papers:
            print(f"\nâœ“ ëª©í‘œ ë…¼ë¬¸ ìˆ˜({total_papers}ê°œ)ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
            break

        print(f"\n[{i}/{len(search_queries)}] ê²€ìƒ‰: {query}")

        try:
            papers = searcher.search_and_fetch(query, max_results=papers_per_query)

            new_papers = 0
            for paper in papers:
                if paper['pmid'] not in seen_pmids:
                    if not is_animal_study(paper):
                        seen_pmids.add(paper['pmid'])
                        paper['search_query'] = query
                        all_papers.append(paper)
                        new_papers += 1

            print(f"   â†’ ìƒˆë¡œìš´ ë…¼ë¬¸ {new_papers}ê°œ ì¶”ê°€")

        except Exception as e:
            print(f"   â†’ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            continue

    return all_papers


def create_default_queries(topic: str) -> List[str]:
    """ê¸°ë³¸ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± - ë” ë‹¤ì–‘í•œ ê²€ìƒ‰ì–´ ì‚¬ìš©"""
    simple_topic = topic.split()[0]

    return [
        # ê¸°ë³¸ ê²€ìƒ‰
        topic,
        simple_topic,
        # ì¹˜ë£Œ ê´€ë ¨
        f"{simple_topic} treatment",
        f"{simple_topic} therapy",
        f"{simple_topic} proton pump inhibitor",
        f"{simple_topic} medication",
        # ì‹ì´ ê´€ë ¨
        f"{simple_topic} diet",
        f"{simple_topic} food",
        f"{simple_topic} nutrition",
        f"{simple_topic} caffeine alcohol",
        # ìƒí™œìŠµê´€ ê´€ë ¨
        f"{simple_topic} lifestyle",
        f"{simple_topic} obesity weight",
        f"{simple_topic} exercise",
        f"{simple_topic} sleep position",
        f"{simple_topic} smoking",
        # ì›ì¸ ê´€ë ¨
        f"{simple_topic} pathophysiology",
        f"{simple_topic} risk factor",
        f"{simple_topic} etiology",
        # ì˜ˆë°©/í•©ë³‘ì¦
        f"{simple_topic} prevention",
        f"{simple_topic} barrett esophagus",
        f"{simple_topic} complications"
    ]


def is_animal_study(paper: dict) -> bool:
    """ë™ë¬¼ ì—°êµ¬ì¸ì§€ í™•ì¸"""
    title_lower = paper['title'].lower()
    abstract_lower = paper['abstract'].lower()

    animal_keywords = [
        'mice', 'mouse', 'rat', 'rats', 'rabbit', 'rabbits',
        'dog', 'dogs', 'cat', 'cats', 'canine', 'feline',
        'porcine', 'bovine', 'animal model', 'in vivo',
        'veterinary', 'murine'
    ]

    for keyword in animal_keywords:
        if keyword in title_lower or keyword in abstract_lower[:500]:
            return True

    return False


def filter_categories(
    categorized_papers: Dict[str, List[Dict]],
    selected_categories: Optional[List[str]]
) -> Dict[str, List[Dict]]:
    """ì„ íƒëœ ì¹´í…Œê³ ë¦¬ë§Œ í•„í„°ë§"""
    if not selected_categories:
        return categorized_papers

    return {
        cat: papers
        for cat, papers in categorized_papers.items()
        if cat in selected_categories
    }


def print_summary(topic: str, papers: List[Dict], categorized: Dict[str, List[Dict]]):
    """ìš”ì•½ ì •ë³´ ì¶œë ¥"""
    print("\n" + "=" * 70)
    print("ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½")
    print("=" * 70)
    print(f"ì£¼ì œ: {topic}")
    print(f"ì´ ë…¼ë¬¸ ìˆ˜: {len(papers)}ê°œ")
    print("\nì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")

    classifier = PaperClassifier()
    for category, cat_papers in sorted(categorized.items(), key=lambda x: -len(x[1])):
        config = classifier.CATEGORY_KEYWORDS.get(category, {'korean_name': category})
        bar = "â–ˆ" * min(len(cat_papers), 30)
        print(f"  {config.get('korean_name', category):8s}: {len(cat_papers):3d}ê°œ {bar}")


def main():
    parser = argparse.ArgumentParser(
        description='ì‹œë¦¬ì¦ˆ ë¸”ë¡œê·¸ ìë™ ìƒì„± íŒŒì´í”„ë¼ì¸',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  python generate_series.py "ì—­ë¥˜ì„±ì‹ë„ì—¼" --total 50
  python generate_series.py "ë¹„íƒ€ë¯¼D ê²°í•" --categories cause,treatment,diet
  python generate_series.py "GERD" --skip-web-search --total 30
        """
    )

    parser.add_argument('topic', type=str, help='ê²€ìƒ‰í•  ì£¼ì œ (ì˜ˆ: "ì—­ë¥˜ì„±ì‹ë„ì—¼", "GERD")')
    parser.add_argument('--total', type=int, default=50, help='ëª©í‘œ ë…¼ë¬¸ ê°œìˆ˜ (ê¸°ë³¸ê°’: 50)')
    parser.add_argument('--categories', type=str, default=None,
                        help='ìƒì„±í•  ì¹´í…Œê³ ë¦¬ (ì‰¼í‘œ êµ¬ë¶„). ì˜ˆ: cause,treatment,diet')
    parser.add_argument('--skip-web-search', action='store_true',
                        help='ì›¹ ê²€ìƒ‰ ì—°ê´€ í‚¤ì›Œë“œ ì¶”ì¶œ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--skip-generation', action='store_true',
                        help='ë¸”ë¡œê·¸ ê¸€ ìƒì„± ê±´ë„ˆë›°ê¸° (ë…¼ë¬¸ ìˆ˜ì§‘ë§Œ)')
    parser.add_argument('--output-dir', type=str, default='output',
                        help='ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: output)')

    args = parser.parse_args()

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(args.output_dir, exist_ok=True)

    print("\n" + "=" * 70)
    print("ğŸ“š ì‹œë¦¬ì¦ˆ ë¸”ë¡œê·¸ ìë™ ìƒì„± íŒŒì´í”„ë¼ì¸")
    print("=" * 70)
    print(f"ì£¼ì œ: {args.topic}")
    print(f"ëª©í‘œ ë…¼ë¬¸: {args.total}ê°œ")
    print(f"ì›¹ ê²€ìƒ‰: {'ê±´ë„ˆëœ€' if args.skip_web_search else 'ì‚¬ìš©'}")
    if args.categories:
        print(f"ì„ íƒ ì¹´í…Œê³ ë¦¬: {args.categories}")
    print("=" * 70)

    # API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    try:
        Config.validate()
        from anthropic import Anthropic
        anthropic_client = Anthropic(api_key=Config.ANTHROPIC_API_KEY)
        print("âœ“ Anthropic API ì—°ê²° ì„±ê³µ")
    except Exception as e:
        print(f"âš ï¸ Anthropic API ì—°ê²° ì‹¤íŒ¨: {str(e)}")
        print("   ë¸”ë¡œê·¸ ê¸€ ìƒì„±ì´ ì œí•œë©ë‹ˆë‹¤.")
        anthropic_client = None

    # ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
    searcher = PubMedSearcher(
        email=Config.PUBMED_EMAIL,
        api_key=Config.PUBMED_API_KEY
    )
    keyword_extractor = WebSearchKeywordExtractor()

    # 1ë‹¨ê³„: ë…¼ë¬¸ ìˆ˜ì§‘
    print("\n" + "=" * 70)
    print("ğŸ“– 1ë‹¨ê³„: ë…¼ë¬¸ ìˆ˜ì§‘")
    print("=" * 70)

    papers = search_with_related_keywords(
        searcher,
        keyword_extractor,
        args.topic,
        args.total,
        skip_web_search=args.skip_web_search
    )

    if not papers:
        print("\nâŒ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    print(f"\nâœ… ì´ {len(papers)}ê°œ ë…¼ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ!")

    # 2ë‹¨ê³„: ë…¼ë¬¸ ë¶„ë¥˜
    print("\n" + "=" * 70)
    print("ğŸ·ï¸ 2ë‹¨ê³„: ë…¼ë¬¸ ë¶„ë¥˜")
    print("=" * 70)

    classifier = PaperClassifier(anthropic_client=anthropic_client)
    categorized = classifier.classify_papers(papers)

    # ì¹´í…Œê³ ë¦¬ í•„í„°ë§
    if args.categories:
        selected = [c.strip() for c in args.categories.split(',')]
        categorized = filter_categories(categorized, selected)

    # ìš”ì•½ ì¶œë ¥
    print_summary(args.topic, papers, categorized)

    # ì¤‘ê°„ ê²°ê³¼ ì €ì¥ (JSON)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    safe_topic = args.topic.replace(' ', '_').replace('/', '_')

    json_filename = f"{safe_topic}_papers_{timestamp}.json"
    json_filepath = os.path.join(args.output_dir, json_filename)

    with open(json_filepath, 'w', encoding='utf-8') as f:
        json.dump({
            'topic': args.topic,
            'total_papers': len(papers),
            'categorized': {k: len(v) for k, v in categorized.items()},
            'papers': papers
        }, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“„ ë…¼ë¬¸ ë°ì´í„° ì €ì¥: {json_filename}")

    # 3ë‹¨ê³„: ë¸”ë¡œê·¸ ê¸€ ìƒì„±
    if not args.skip_generation:
        print("\n" + "=" * 70)
        print("âœï¸ 3ë‹¨ê³„: ì‹œë¦¬ì¦ˆ ë¸”ë¡œê·¸ ê¸€ ìƒì„±")
        print("=" * 70)

        # ì¹´í…Œê³ ë¦¬ë³„ ìµœì†Œ ë…¼ë¬¸ ìˆ˜ í™•ì¸
        valid_categories = {
            cat: papers_list
            for cat, papers_list in categorized.items()
            if len(papers_list) >= Config.MIN_PAPERS_PER_CATEGORY
        }

        if not valid_categories:
            print(f"\nâš ï¸ ê° ì¹´í…Œê³ ë¦¬ì— ìµœì†Œ {Config.MIN_PAPERS_PER_CATEGORY}ê°œ ì´ìƒì˜ ë…¼ë¬¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            print("   ë…¼ë¬¸ ìˆ˜ê°€ ë¶€ì¡±í•œ ì¹´í…Œê³ ë¦¬:")
            for cat, papers_list in categorized.items():
                if len(papers_list) < Config.MIN_PAPERS_PER_CATEGORY:
                    print(f"   - {cat}: {len(papers_list)}ê°œ")
            sys.exit(1)

        generator = SeriesBlogGenerator(
            anthropic_client=anthropic_client,
            output_dir=args.output_dir
        )

        results = generator.generate_all_posts(args.topic, valid_categories)

        if results:
            print("\n" + "=" * 70)
            print("âœ… ìƒì„± ì™„ë£Œ!")
            print("=" * 70)
            print(f"\nğŸ“ ìƒì„±ëœ íŒŒì¼ ({len(results)}ê°œ):")
            for result in results:
                print(f"   - {result['filename']}")
                print(f"     ({result['korean_name']}, ë…¼ë¬¸ {result['paper_count']}ê°œ)")

            # ì‹œë¦¬ì¦ˆ ì¸ë±ìŠ¤ íŒŒì¼ ìœ„ì¹˜
            index_file = f"{safe_topic}_series_index.json"
            print(f"\nğŸ“‹ ì‹œë¦¬ì¦ˆ ì¸ë±ìŠ¤: {index_file}")
        else:
            print("\nâš ï¸ ìƒì„±ëœ ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print("\nâ­ï¸ ë¸”ë¡œê·¸ ê¸€ ìƒì„±ì„ ê±´ë„ˆë›°ì—ˆìŠµë‹ˆë‹¤.")

    print("\n" + "=" * 70)
    print("ğŸ‰ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    print("=" * 70)

    return {
        'success': True,
        'topic': args.topic,
        'paper_count': len(papers),
        'categories': list(categorized.keys())
    }


if __name__ == "__main__":
    try:
        result = main()
        sys.exit(0 if result['success'] else 1)
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(130)
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
